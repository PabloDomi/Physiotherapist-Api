{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIBLIOTECAS   \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from scipy.signal import savgol_filter, find_peaks\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExerciseAnalyzer:\n",
    "    \"\"\"\n",
    "    Clase para analizar ejercicios basados en datos de posiciones de landmarks.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, exercise_key, fps=30, window_length=75, polyorder=5, default_distance=9, default_prominence=0.4, default_distance_thresh=90):\n",
    "        self.exercise_dict = self._construct_exercise_dict()\n",
    "        self.data_path = data_path\n",
    "        self.exercise_key = exercise_key\n",
    "        self.fps = fps\n",
    "        self.window_length = window_length\n",
    "        self.polyorder = polyorder\n",
    "        \n",
    "        # Variables específicas del ejercicio con valores por defecto\n",
    "        exercise_params = self.exercise_dict.get(exercise_key, {})\n",
    "        self.distance = exercise_params.get('distance', default_distance)\n",
    "        self.prominence = exercise_params.get('prominence', default_prominence)\n",
    "        self.distance_thresh = exercise_params.get('distance_thresh', default_distance_thresh)\n",
    "        self.df = None\n",
    "        self.peaks_dict = None\n",
    "        self.intervals_dict = None\n",
    "        self.results = None\n",
    "        self.total_intervals = None\n",
    "        self.total_peaks = None\n",
    "        self.read_data()\n",
    "        self.calculate_velocity()\n",
    "        self.calculate_smooth_velocity()\n",
    "        self.calculate_peaks()\n",
    "        self.apply_group_peaks_to_dict()\n",
    "        self.create_intervals()\n",
    "        self.total_intervals = self.final_intervals()\n",
    "        self.total_peaks = self.final_peaks()\n",
    "\n",
    "    # DICCIONARIO CON LA RELACION LANDMARK-EJERCICIO\n",
    "    def _construct_exercise_dict(self):\n",
    "        exercise_dict = {\n",
    "            \"squat\" : {\n",
    "                \"x_axis\": [],\n",
    "                \"y_axis\": [2, 5, 11, 12],\n",
    "                \"z_axis\": [],\n",
    "                \"distance\": 9,\n",
    "                \"prominence\": 0.3,\n",
    "                \"distance_thresh\": 130},\n",
    "            \"lateral_shoulder\" : {\n",
    "                \"x_axis\": [],\n",
    "                \"y_axis\": [15, 16, 17, 18, 19, 20, 21, 22],\n",
    "                \"z_axis\": [],\n",
    "                \"distance\": 9,\n",
    "                \"prominence\": 0.4,\n",
    "                \"distance_thresh\": 90},\n",
    "            \"pull_up\" : {\n",
    "                \"x_axis\": [],\n",
    "                \"y_axis\": [11, 12, 10, 9, 0],\n",
    "                \"z_axis\": [],\n",
    "                \"distance\": 10,\n",
    "                \"prominence\": 0.4,\n",
    "                \"distance_thresh\": 130},\n",
    "            \"deadlift\" : {\n",
    "                \"x_axis\": [],\n",
    "                \"y_axis\": [0, 11, 12, 15, 16],\n",
    "                \"z_axis\": [],\n",
    "                \"distance\": 7,\n",
    "                \"prominence\": 0.4,\n",
    "                \"distance_thresh\": 130},\n",
    "            \"hammer_curl\" : {\n",
    "                \"x_axis\": [],\n",
    "                \"y_axis\": [15, 16, 17, 18],\n",
    "                \"z_axis\": [],\n",
    "                \"distance\": 9,\n",
    "                \"prominence\": 0.5,\n",
    "                \"distance_thresh\": 90},\n",
    "            \"lat_pulldown\" : {\n",
    "                \"x_axis\": [],\n",
    "                \"y_axis\": [13, 14, 15, 16],\n",
    "                \"z_axis\": [],\n",
    "                \"distance\": 9,\n",
    "                \"prominence\": 0.4,\n",
    "                \"distance_thresh\": 120},\n",
    "            \"shoulder_press\" : {\n",
    "                \"x_axis\": [],\n",
    "                \"y_axis\": [13, 14, 15, 16, 17, 18],\n",
    "                \"z_axis\": [],\n",
    "                \"distance\": 9,\n",
    "                \"prominence\": 0.4,\n",
    "                \"distance_thresh\": 110},\n",
    "            \"tricep_dips\" : {\n",
    "                \"x_axis\": [],\n",
    "                \"y_axis\": [11,12,0,10,9],\n",
    "                \"z_axis\": [],\n",
    "                \"distance\": 9,\n",
    "                \"prominence\": 0.4,\n",
    "                \"distance_thresh\": 125},\n",
    "            \"tricep_pushdown\" : {\n",
    "                \"x_axis\": [],\n",
    "                \"y_axis\": [15,16,17,18,19,20],\n",
    "                \"z_axis\": [],\n",
    "                \"distance\": 9,\n",
    "                \"prominence\": 0.4,\n",
    "                \"distance_thresh\": 110},\n",
    "            \"push_up\" : {\n",
    "                \"x_axis\": [],\n",
    "                \"y_axis\": [0, 11, 12, 9, 10],\n",
    "                \"z_axis\": [],\n",
    "                \"distance\": 9,\n",
    "                \"prominence\": 0.4,\n",
    "                \"distance_thresh\": 125},\n",
    "            }\n",
    "        return exercise_dict\n",
    "\n",
    "\n",
    "    # LECTURA DE DATOS\n",
    "    def read_data(self):\n",
    "        \"\"\"\n",
    "        Read  data from the specified CSV file in data_path.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "        return self.df\n",
    "\n",
    "    # CALCULO DE VELOCIDAD\n",
    "    def calculate_velocity(self):\n",
    "        \"\"\"\n",
    "        Calcula la velocidad en los ejes x, y, z y añade estas velocidades como nuevas columnas en el DataFrame.\n",
    "        \"\"\"\n",
    "        velocity_x = np.zeros(len(self.df))\n",
    "        velocity_y = np.zeros(len(self.df))\n",
    "        velocity_z = np.zeros(len(self.df))\n",
    "        \n",
    "        for landmark in self.df[\"landmark\"].unique():\n",
    "            positions_x = self.df.loc[self.df[\"landmark\"] == landmark, \"x\"].values\n",
    "            positions_y = self.df.loc[self.df[\"landmark\"] == landmark, \"y\"].values\n",
    "            positions_z = self.df.loc[self.df[\"landmark\"] == landmark, \"z\"].values\n",
    "            \n",
    "            dx = np.diff(positions_x)\n",
    "            dy = np.diff(positions_y)\n",
    "            dz = np.diff(positions_z)\n",
    "            \n",
    "            dt = 1 / self.fps\n",
    "            \n",
    "            vx = dx / dt\n",
    "            vy = dy / dt\n",
    "            vz = dz / dt\n",
    "            \n",
    "            vx = np.insert(vx, 0, 0)\n",
    "            vy = np.insert(vy, 0, 0)\n",
    "            vz = np.insert(vz, 0, 0)\n",
    "            \n",
    "            velocity_x[self.df[\"landmark\"] == landmark] = vx\n",
    "            velocity_y[self.df[\"landmark\"] == landmark] = vy\n",
    "            velocity_z[self.df[\"landmark\"] == landmark] = vz\n",
    "        \n",
    "        self.df[\"vx\"] = velocity_x\n",
    "        self.df[\"vy\"] = velocity_y\n",
    "        self.df[\"vz\"] = velocity_z\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    # CALCULO DE VELOCIDAD SUAVIZADA\n",
    "    def calculate_smooth_velocity(self):\n",
    "        \"\"\"\n",
    "        Calculate the velocity in the x, y, z axes and add these velocities as new columns in the DataFrame.\n",
    "        \"\"\"\n",
    "        velocity_x_smooth = np.zeros(len(self.df))\n",
    "        velocity_y_smooth = np.zeros(len(self.df))\n",
    "        velocity_z_smooth = np.zeros(len(self.df))\n",
    "        \n",
    "        # Iterar sobre cada landmark para aplicar el suavizado\n",
    "        for landmark in self.df[\"landmark\"].unique():\n",
    "            # Extraer velocidades para el landmark actual\n",
    "            vx = self.df.loc[self.df[\"landmark\"] == landmark, \"vx\"].values\n",
    "            vy = self.df.loc[self.df[\"landmark\"] == landmark, \"vy\"].values\n",
    "            vz = self.df.loc[self.df[\"landmark\"] == landmark, \"vz\"].values\n",
    "            \n",
    "            # Aplicar filtro de suavizado Savitzky-Golay\n",
    "            vx_smooth = savgol_filter(vx, window_length=self.window_length, polyorder=self.polyorder)\n",
    "            vy_smooth = savgol_filter(vy, window_length=self.window_length, polyorder=self.polyorder)\n",
    "            vz_smooth = savgol_filter(vz, window_length=self.window_length, polyorder=self.polyorder)\n",
    "            \n",
    "            # Insertar las velocidades suavizadas en las posiciones correspondientes en el array temporal\n",
    "            velocity_x_smooth[self.df[\"landmark\"] == landmark] = vx_smooth\n",
    "            velocity_y_smooth[self.df[\"landmark\"] == landmark] = vy_smooth\n",
    "            velocity_z_smooth[self.df[\"landmark\"] == landmark] = vz_smooth\n",
    "        \n",
    "        # Añadir las nuevas columnas suavizadas al DataFrame\n",
    "        self.df[\"vx_smooth\"] = velocity_x_smooth\n",
    "        self.df[\"vy_smooth\"] = velocity_y_smooth\n",
    "        self.df[\"vz_smooth\"] = velocity_z_smooth\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    # CALCULO DE PICOS\n",
    "    def calculate_peaks(self):\n",
    "        \"\"\"\n",
    "        Calculate the velocity in the x, y, z axes and add these velocities as new columns in the DataFrame.\n",
    "        \"\"\"\n",
    "        # Diccionario para almacenar los picos\n",
    "        peaks_dict = {}\n",
    "        \n",
    "        # Iterar sobre cada landmark único\n",
    "        for landmark in self.df[\"landmark\"].unique():\n",
    "            peaks_dict[int(landmark)] = {}\n",
    "            \n",
    "            # Iterar sobre cada eje de velocidad\n",
    "            for axis in [\"vx\", \"vy\", \"vz\"]:\n",
    "                # Extraer los datos suavizados para el landmark actual y el eje\n",
    "                data = self.df.query(f\"landmark == {landmark}\")[f\"{axis}_smooth\"].tolist()\n",
    "                \n",
    "                # Encontrar los picos en los datos suavizados\n",
    "                peaks, _ = find_peaks(data, distance=self.distance, prominence=self.prominence)\n",
    "                \n",
    "                # Almacenar los picos en el diccionario\n",
    "                peaks_dict[int(landmark)][f\"{axis}_sm\"] = peaks\n",
    "        \n",
    "        self.peaks_dict = peaks_dict\n",
    "        return self.peaks_dict\n",
    "    \n",
    "    # AGRUPAR PICOS\n",
    "    def _group_peaks(self, peaks, distance_thresh):\n",
    "        \"\"\"\n",
    "        Groups peaks that are close to each other into sections.\n",
    "        \"\"\"\n",
    "        if len(peaks) == 0:\n",
    "            return []\n",
    "        \n",
    "        peaks = np.sort(peaks)\n",
    "        peaks = [int(p) for p in peaks]\n",
    "        \n",
    "        sections = []\n",
    "        section_update = [peaks[0]]\n",
    "        \n",
    "        for i in range(1, len(peaks)):\n",
    "            if peaks[i] - peaks[i-1] <= distance_thresh:\n",
    "                section_update.append(peaks[i])\n",
    "            else:\n",
    "                sections.append(section_update)\n",
    "                section_update = [peaks[i]]\n",
    "        \n",
    "        if section_update:\n",
    "            sections.append(section_update)\n",
    "        \n",
    "        return sections\n",
    "\n",
    "    # APLICAR AGRUPAR PICOS\n",
    "    def apply_group_peaks_to_dict(self):\n",
    "        \"\"\"\n",
    "        Apply the _group_peaks function to the peaks in the dictionary and store the results in a new dictionary.\n",
    "        \"\"\"\n",
    "        grouped_dict = {}\n",
    "        \n",
    "        for key, value in self.peaks_dict.items():\n",
    "            grouped_dict[key] = {}\n",
    "            \n",
    "            for subkey, peaks in value.items():\n",
    "                if len(peaks) > 0:  # Asegurar que no es un array vacío\n",
    "                    grouped_dict[key][subkey] = self._group_peaks(peaks, self.distance_thresh)\n",
    "                else:             \n",
    "                    grouped_dict[key][subkey] = []\n",
    "        \n",
    "        self.peaks_dict = grouped_dict\n",
    "        return self.peaks_dict\n",
    "\n",
    "    # CREAR INTERVALOS\n",
    "    def create_intervals(self):\n",
    "        \"\"\"\n",
    "        Create intervals from the grouped peaks and store them in a dictionary.\n",
    "        \"\"\"\n",
    "        intervals_dict = {}\n",
    "        \n",
    "        for key, axes_dict in self.peaks_dict.items():\n",
    "            intervals_dict[key] = {}\n",
    "            \n",
    "            for axis, lists in axes_dict.items():\n",
    "                intervals_dict[key][axis] = []\n",
    "                \n",
    "                for peak_list in lists:\n",
    "                    if len(peak_list) == 0:\n",
    "                        continue\n",
    "                    elif len(peak_list) == 1:\n",
    "                        continue\n",
    "                    else:\n",
    "                        distances = np.diff(peak_list)\n",
    "                        avg_distance = int(round(np.mean(distances)))\n",
    "                        \n",
    "                        new_start = int(peak_list[0] - 0.5 * avg_distance)\n",
    "                        new_end = int(peak_list[-1] + 0.5 * avg_distance)\n",
    "                        intervals_dict[key][axis].append([new_start, new_end])\n",
    "        \n",
    "        self.intervals_dict = intervals_dict\n",
    "        return self.intervals_dict\n",
    "\n",
    "    ###### INTERVALOS ######\n",
    "    def _calculate_mean_intervals(self, intervals):\n",
    "        \"\"\"\n",
    "        Calculate the mean intervals and round to the nearest integer.\n",
    "        \"\"\"\n",
    "        if not intervals:\n",
    "            return [0, 0]\n",
    "        \n",
    "        sum_start = sum(interval[0] for interval in intervals)\n",
    "        sum_end = sum(interval[1] for interval in intervals)\n",
    "        n = len(intervals)\n",
    "        \n",
    "        media_start = round(sum_start / n)\n",
    "        media_end = round(sum_end / n)\n",
    "        \n",
    "        return [media_start, media_end]\n",
    "    \n",
    "    def final_intervals(self):\n",
    "        \"\"\"\n",
    "        Calculate the final intervals for the given exercise.\n",
    "        \"\"\"\n",
    "        # Obtener los valores de exercise_dict para la clave dada\n",
    "        exercise_values = self.exercise_dict[self.exercise_key]\n",
    "\n",
    "        # Crear una lista para almacenar los intervalos\n",
    "        final_intervals = []\n",
    "\n",
    "        for axis, landmarks in exercise_values.items():\n",
    "            if isinstance(landmarks, list) and landmarks:\n",
    "                grouped_intervals = []\n",
    "                for landmark in landmarks:\n",
    "                    if landmark in self.intervals_dict and f'v{axis[0]}_sm' in self.intervals_dict[landmark]:\n",
    "                        grouped_intervals.append(self.intervals_dict[landmark][f'v{axis[0]}_sm'])\n",
    "\n",
    "                if grouped_intervals:\n",
    "                    # Agrupar intervalos por su posición (primeros con primeros, segundos con segundos, etc.)\n",
    "                    overall_intervals = list(zip(*grouped_intervals))\n",
    "                    for group in overall_intervals:\n",
    "                        mean_interval = self._calculate_mean_intervals(group)\n",
    "                        final_intervals.append(mean_interval)\n",
    "\n",
    "        self.total_intervals = final_intervals\n",
    "        return self.total_intervals\n",
    "\n",
    "    \n",
    "    ###### PICOS ######\n",
    "    def _calculate_mean_peaks(self, peaks_list):\n",
    "        \"\"\"\n",
    "        Calculate the mean of the peaks and round to the nearest integer.\n",
    "        \"\"\"\n",
    "        # Filtrar para quedarse con las listas que tengan el mayor número de elementos\n",
    "        max_len = max(len(peaks) for peaks in peaks_list)\n",
    "        filtered_peaks_list = [peaks for peaks in peaks_list if len(peaks) == max_len]\n",
    "\n",
    "        # Calcular la media de cada conjunto de picos\n",
    "        transposed_peaks = list(zip(*filtered_peaks_list))\n",
    "        mean_peaks = [round(np.mean(peak)) for peak in transposed_peaks]\n",
    "        return mean_peaks\n",
    "\n",
    "    def final_peaks(self):\n",
    "        \"\"\"\n",
    "        Calculate the final peaks for the given exercise.\n",
    "        \"\"\"\n",
    "        # Obtener los valores de exercise_dict para la clave dada\n",
    "        exercise_values = self.exercise_dict[self.exercise_key]\n",
    "        \n",
    "        # Crear una lista para almacenar los picos medios\n",
    "        final_mean_peaks = []\n",
    "\n",
    "        for axis, landmarks in exercise_values.items():\n",
    "            if isinstance(landmarks, list) and landmarks:\n",
    "                # Crear una lista para almacenar todos los picos de cada eje\n",
    "                all_peaks = []\n",
    "                for landmark in landmarks:\n",
    "                    if landmark in self.peaks_dict and self.peaks_dict[landmark][f'v{axis[0]}_sm']:\n",
    "                        all_peaks.append(self.peaks_dict[landmark][f'v{axis[0]}_sm'])\n",
    "                \n",
    "                # Si hay picos en este eje, calcular la media de cada conjunto de picos\n",
    "                if all_peaks:\n",
    "                    num_intervals = len(max(all_peaks, key=len))\n",
    "                    for i in range(num_intervals):\n",
    "                        interval_group = [peaks[i] if len(peaks) > i else np.nan for peaks in all_peaks]\n",
    "                        if all(isinstance(x, list) for x in interval_group):\n",
    "                            final_mean_peaks.append(self._calculate_mean_peaks(interval_group))\n",
    "                        else:\n",
    "                            final_mean_peaks.append(interval_group)\n",
    "        \n",
    "        self.total_peaks = final_mean_peaks\n",
    "        return self.total_peaks\n",
    "\n",
    "    def num_peaks_per_interval(self):\n",
    "        result = {}\n",
    "        \n",
    "        for i, interval in enumerate(self.total_intervals):\n",
    "            start, end = interval\n",
    "            counter = sum(1 for peak in self.total_peaks if start <= peak <= end)\n",
    "            result[i] = {tuple(interval): counter}\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def analyze_exercise(self):\n",
    "        \"\"\"\n",
    "        Create a dictionary that counts, for each interval, the number of peaks and converts them to time,\n",
    "        rounding to the third decimal digit.\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        \n",
    "        for i, (interval, peaks) in enumerate(zip(self.total_intervals, self.total_peaks)):\n",
    "            start, end = interval\n",
    "            time_interval = [round(start / self.fps, 3), round(end / self.fps, 3)]\n",
    "            \n",
    "            # # Asegurar que peaks es una lista\n",
    "            # if not isinstance(peaks, list):\n",
    "            #     peaks = [peaks]\n",
    "                \n",
    "            time_peaks = [round(peak / self.fps, 3) for peak in peaks]\n",
    "            result[i + 1] = {tuple(time_interval): time_peaks}\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def create_graph(self, lndmrk=None):\n",
    "        # Obtener el landmark por defecto si no se especifica uno\n",
    "        if lndmrk is None:\n",
    "            exercise_values = self.exercise_dict[self.exercise_key]\n",
    "            for axis, landmarks in exercise_values.items():\n",
    "                if landmarks:\n",
    "                    lndmrk = landmarks[0]\n",
    "                    break\n",
    "        \n",
    "        if lndmrk is None:\n",
    "            print(\"No landmarks available for the graph.\")\n",
    "            return\n",
    "        \n",
    "        # Crear la figura y el eje\n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "        # Datos suavizados\n",
    "        smooth_data = self.df.query(f\"landmark == {lndmrk}\")['vy_smooth'].reset_index(drop=True)\n",
    "        \n",
    "        # Filtrar valores no finitos\n",
    "        smooth_data = smooth_data[np.isfinite(smooth_data)]\n",
    "        \n",
    "        # Graficar los datos suavizados\n",
    "        line, = ax.plot(smooth_data, label='Señal suavizada')\n",
    "        \n",
    "        # Patch para la leyenda del color verde y rojo\n",
    "        green_patch = Patch(color='green', alpha=0.3, label='Intervalos')\n",
    "        red_patch = Patch(color='red', alpha=0.5, label='Picos')\n",
    "        \n",
    "        # Colorear los intervalos especificados en verde\n",
    "        for start, end in self.total_intervals:\n",
    "            if np.isfinite(start) and np.isfinite(end):\n",
    "                ax.axvspan(start, end, color='green', alpha=0.3)\n",
    "        \n",
    "        # Graficar los picos detectados\n",
    "        flat_peaks = [item for sublist in self.total_peaks for item in sublist if isinstance(item, (int, float)) and np.isfinite(item)]  # Aplanar la lista de listas y filtrar no numéricos y NaNs\n",
    "\n",
    "        ax.vlines(flat_peaks, ymin=min(smooth_data), ymax=max(smooth_data), color='red', label='Picos', linestyles='dashed')\n",
    "\n",
    "        # Añadir el número de la repetición sobre cada línea\n",
    "        for i, peak in enumerate(flat_peaks):\n",
    "            if np.isfinite(peak):\n",
    "                ax.text(peak, smooth_data[int(peak)], f'{i + 1}', verticalalignment='bottom', horizontalalignment='center')\n",
    "        \n",
    "        # Añadir leyenda y etiqueta\n",
    "        ax.legend(handles=[line, green_patch, red_patch])\n",
    "\n",
    "        # Añadir título\n",
    "        fig.suptitle(f'Visualización de velocidad suavizada con intervalos coloreados y repeticiones detectadas (landmark {lndmrk})')\n",
    "\n",
    "        # Mostrar la gráfica\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruta al arhivo CSV\n",
    "data_path = r\"C:\\Users\\ARMCO\\Documents\\PROYECTOS\\Anaphys\\notebooks\\output_2.csv\"\n",
    "\n",
    "# Crear una instancia de la clase ExerciseAnalyzer\n",
    "ejemplo = ExerciseAnalyzer(data_path, \"squat\")\n",
    "\n",
    "# Analizar el ejercicio\n",
    "ejemplo.analyze_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la gráfica del ejercicio\n",
    "ejemplo.create_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
